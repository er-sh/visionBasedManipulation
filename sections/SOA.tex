Vision based robot control is generally classified into position-based visual servo (PBVS) and image-based visual servo (IBVS) \cite{janabi2011comparison}. IBVS is more robust that PBVS w.r.t. uncertainties and disturbances affecting the model of the robot, as well as the calibration of the camera \cite{malis2003robustness}. Besides, traditional IBVS methods need to know an interaction matrix for which the key parameter is  the depth value for each feature point in each iteration of control loop. Thus, many researchers use a stereo camera to determine every depth values for each feature point immediately \cite{wang2014modified}, or use other techniques \cite{luo2014hybrid}, \cite{zhong2015robots}, \cite{kosmopoulos2011robust}. Nevertheless, in some recent works such as \cite{tongloy2016image}, there has been no need for the interaction matrix. Apart from pure PBVS and IBVS methods, there exists also the combination of both which is called 2-1/2-D visual servo \cite{malis19992}. \par 
Depending on the mounting position of camera, there are two types of visual servo systems: eye-to-hand and eye-in-hand \cite{dong2015position}. Although in eye-to-hand mode, problems associated with moving camera is removed, other issues arise such as camera-to-robot coordinate transformation, occlusion of interested features due to the manipulator, and the need for calibration \cite{smith1996vision}. \par 
Information from vision can be used for several control tasks such as object manipulation. For this, object can be recognized via visual model-based object recognition methods as in \cite{nuchter2008towards} and \cite{lee2008toward}, or methods such \cite{yamazaki2006grasp} and \cite{ohno2011unknown} in which the object model is initially unknown. Vision can be combined with other sensors to improve the manipulation skills of robot, such as in \cite{bimbo2013combining} and \cite{li2013integrating} that tactile feedback was used, or in \cite{prats2009vision} and \cite{hebert2011fusion} that force/torque sensor was used in fusion to visual data. Moreover, in some works vision is used to enhance other sensed/estimated data such as force as in \cite{pham2015towards}. Furthermore, there have been several approached to improve the information gathered from the visual sensing in case of uncertainties such as adaptive visual tracking in \cite{wang2015adaptive}.  \par 
Recent progresses in robotics have brought more challenging capabilities using visual servoing. In \cite{bauml2011catching}, robot equipped with a stereo camera is able to perform motion needed to catch thrown balls. In \cite{kim2014catching} and \cite{salehian2016dynamical}, robot can even catch flying objects of various forms, such as plastic bottle and brick. In a more recent work \cite{shimada2016high}, high-speed 3D sensor is mounted on an active vision system that leads to more accurate recognition of a moving object. \par 
Vision has been fused with more complicated tasks such as soft object manipulation. Tracking flexible objects has been in done in \cite{salzmann2007deformable} via feature detection, in \cite{schulman2013tracking} via background subtraction and in \cite{elbrechter2011bi} via utilization of markers. Also in the more recent work of \cite{bodenhagen2014adaptable} this is done with no processing of visual data. Thus, robots are capable of doing complex motions such as rotational manipulation of pizza dough in \cite{yamakawa2013dynamic}. \par 
Dexterous manipulators can also use visual servoing to be able to perform more advanced motions such as juggling and kendama catching motions in \cite{kizaki2012two} and \cite{namiki2014ball}. \cite{lippiello2013visual} has proposed a method for fast visual grasping of unknown objects with multi-fingered robotic hand. And \cite{calli2016vision} has proposed a method even for underactuated grippers by combining the robustness of visual servoing methods and precision manipulation primitives. \par 
In the area of Humanoid robots, recently in \cite{claudio2016vision} both IBVS and PBVS have been used for gaze control and for generic object grasping respectively. This led to some real applications such as box grasping and can grasping for the humanoid robot Romeo. Another humanoid robot REEM could also do some tasks such as box grasping (\cite{agravante2013visual}) via visual servoing. More specifically, in \cite{tan2015integrated} some applications such as surgical tools manipulation has been done by Humanoid robots. Moreover, visual servoing has been also used for assistance in some works such as \cite{troniak2013charlie} for locating points of interests and object manipulation and in \cite{quintero2015vibi} for replacing direct joystick motor control interface in commercial wheelchair mounted assitive robotic manipulator with a human-robot interface based on visual selection. \par 
Recently, vision based control has been used more often in other robotic areas such as underwater robots \cite{huang2016underwater}, agriculture \cite{michaels2015vision}, and even nanorobotics such as \cite{shi2017vision}, in which a nanorobotic manipulation system allows for automated pick-up of carbon nanotube based on visual feedback. Moreover, in aerial robotics, there have been some recent works such as \cite{santamaria2017uncalibrated}, in which an uncalibrated IBVS strategy has been used for manipulation, or \cite{lippiello2016hybrid}, where the combination of IBVS and PBVS was used, or \cite{suphachart2016vision}, in which an FPGA-based method was used for aerial manipulation. \par 
Finally, visual servo based manipulation methods have been also recently utilized in areas including teleoperation such as \cite{selvaggio2016vision} and \cite{chenf2016framework}, multi-robot scenarios such as \cite{wang2016image} in which uncertainties in kinematics and dynamics of robot manipulators are considered, and also in combination of machine learning such as \cite{das2016learning}, where skill learning through Symbolic Encoding rather than trajectory encoding was used. 