\section{general}
\subsection{2016 - An image-based visual servo control system based on an eye-in-hand monocular camera for autonomous robotic grasping}
\begin{itemize}
\item No use of interaction matrix as traditional method
\item In visual servo system, information from a vision system is used as feedback signals in a closed-loop control system to control robot motion in real-time [1]
\item An eye-to-hand camera is installed beside robot while an eye-in-hand camera is mounted on end-effector [2]
\item An eye-to-hand removes problems associated with a moving camera however it may have some issues such as the camera-to-robot coordinate transformation, manipulator occlude interested features and calibration arise [3]. 
\item An eye-in-hand camera can change vision position so that the coordinate transformation is relatively simple however it suffers from some problem such as interested features is outside from the field of view while a camera is moving.
\item Traditional image-based visual servo needs to know an interaction matrix. 
\item  Our method is based on the point error, area error of features and simple depth estimation
\item A key parameter to determine an interaction matrix is the depth value for each feature point in each iteration of control loop. Therefore, many researchers use a stereo camera to determine every depth values for each feature point immediately. [4] 
\item In some research works [6] they used monocular camera and interaction matrix without changing the pitch and roll of camera and interested features.They also estimate depth value from Kalman Filter.
\item In some work [7], they used monocular camera and estimate interaction matrix from
Kalman-neural-network filtering. 
\item In addition, Kosmopoulus [8] used monocular camera and estimate interaction matrix from local estimation.
\end{itemize}
1: \textit{2000 - A new hybrid image-based visual servo control scheme} \\
2: \textit{2015 - Position-based visual servo control of autonomous robotic manipulators} \\
3: \textit{1996 - Vision-guided robotic grasping: issues and experiments} \\
4: \textit{2014 - A modified image-based visual servo controller with hybrid camera configuration for robust robotic grasping,} \\
5: \textit{2012 - Combining stereo vision and fuzzy image based visual servoing for autonomous object grasping using a 6-DOF manipulator} \\
6: \textit{2014 - Hybrid Eye-to-hand and Eye-in-hand visual servo system for parallel robot conveyor object tracking and fetching} \\
7: \textit{2015 - Robots visual servo control with features constraint employing Kalman-neural-network filtering scheme} \\
8: \textit{2011 - Robust Jacobian matrix estimation for image-based
visual servoing}



\subsection{Towards Force Sensing from Vision: Observing Hand-Object Interactions to Infer Manipulation Forces}
\begin{itemize}
\item a single RGB-D camera for model-based visual
tracking to estimate the object's pose together with that
of the hand manipulating it throughout the motion. 
\end{itemize}

\subsection{2016 - Learning Object Manipulation from Demonstration through Vision for the 7-DOF Barrett WAM}
\begin{itemize}
\item Skill learning through Symbolic Encoding rather than trajectory encoding. 
\item Skill learning part was mostly inspired by [1]
\end{itemize}
1: \textit{2013 - Visuospatial skill learning for object reconfiguration
tasks}


\subsection{2011 - Unknown Object Modeling on the Basis of Vision and Pushing Manipulation}
\begin{itemize}
\item modeling unknown objects in human life environment using robot technologies. Using vision sensors, a manipulator, and a tactile sensor
\item During the pushing manipulation, the object shape and the motion are measured by using a 3-D range camera and a stereo camera
\item Visual model-based object recognition is an important research in robotics. There are a lot of researches in this field [1], [2], [3], [4], [5], [6].
\item modeling an unknown object on a desk using
a monocular camera [6]
\item proposed a method for estimating the center of friction using visual information [7]
\item  rotating the
object by 360 degrees with the finger of the manipulator
\end{itemize}
1: \textit{2002 -  3D object recognition in cluttered
environments by segment-based stereo vision} \\
2: \textit{2008 - Towards semantic maps for mobile robots} \\
3: \textit{2006 - 3-D object map building using dense object models with SIFT-based recognition features} \\
4: \textit{2008: Toward human-like real-time manipulation: From perception to motion planning} \\
5: \textit{2006 - Vision-based scene representation for 3D interaction of service robots} \\
6: \textit{2006 - A grasp planning for picking
up an unknown object for a mobile manipulator} \\
7: \textit{1992 - Estimation of the Friction Distribution
from Pushing an Object}

\subsection{2013 - Integrating vision, haptics and proprioception into a feedback controller for in-hand manipulation of unknown objects}
\begin{itemize}
\item Employs a fast feedback loop based on visual and tactile feedback to perform robust manipulation even in the presence of unexpected slippage or rolling
\item In contrast to
traditional manipulation strategies, which require a lot of
[1] information about the object and which plan in an offline
fashion, our method plans in an online fashion and employs minimal object and world information. 
\item Because the proposed method does not depend on the object geometry, it can be easily employed for differently shaped objects as was shown in the accompanying video
\end{itemize}
1: \textit{2011 - A modular high-speed tactile sensor for human manipulation research}

\subsection{2013 - Combining Touch and Vision for the Estimation of an Object’s Pose During Manipulation}
\begin{itemize}
\item This paper presents a method to use the robot’s sense of touch to refine the knowledge of a manipulated object’s pose from an initial estimate provided by vision. 
\item Among the most relevant publications that use this approach are the works by Allen et al [1], who combined vision, force and tactile sensing to portray how these different sensing modalities could complement each other.
\item [2] explored a method
to localise an object using Scaling Series.
\item [3] combined tactile, force and vision information to locate and open a door handle and compared the performance under different sensing settings (only force, force and vision and tactile, force and vision), where the combination of the three modalities proved to outperform the other two, which is analogue to the previously mentioned results
\item  Estimation of an object’s pose combining stereo vision and a force-torque sensor mounted on the wrist of a robot was reported by Hebert et al [4], who also used the joint position to estimate the location of the fingers with respect to the object’s faces.
\end{itemize}
1: \textit{1999 - Integration of
Vision , Force and Tactile Sensing for Grasping} \\
2: \textit{2011 - Global localization of objects via
touch} \\
3: \textit{2009 - Vision-tactile-force integration and robot physical interaction}
4: \textit{Fusion of stereo vision, forcetorque,
and joint sensors for estimation of in-hand object location}

\section{High-speed tracking}
\subsection{2016 - High-speed 3-D measurement of a moving object with visual servo}
\begin{itemize}
\item A robot equipped with a stereo camera was able to perform the motions needed to prepare coffee and to catch thrown balls. [1]
\item To catch flying objects, it is necessary not only to estimate the trajectories of the objects in a short time but also to rapidly and accurately control the motion of robot arms.
\item In some works such as [2] and [3] robot could catch flying objects of various forms, such as a plastic bottle, a tennis racket, and a brick, by estimating the trajectories via machine learning, but recognition of the type of object requires markers to be attached to the object
\item Quick manipulation via high-speed stereo vision system that can measure the 3-D centroid position of a target at 500 frames per second. Dexterous actions, such as juggling and kendama catching motions [4], [5]. But, difficult to handle an object with a complex shape.
\item  We have developed a system in which a high-speed 3-D sensor is mounted on a 2-axis active vision system. As a result, the measurement range becomes wider, and 3-D recognition of a
rapidly moving object becomes more accurate because rapid
tracking
\end{itemize}
1: \textit{2011 -  Catching flying balls and preparing coffee: humanoid RollinfJustin performs dynamic and sensitive tasks} \{\ref{humanoid}\} \\
2: \textit{2014 -  Catching Objects in Flight} \\
3: \textit{2016 - A dynamical system
approach for softly catching a flying object: theory and experiment} \\
4: \textit{2012 - Two ball
juggling with high-speed hand-arm and high-speed vision system} \\
5: \textit{2014 - Ball catching in kendama game by estimating
grasp conditions based on a high-speed vision system and tactile
sensors}

\section{Nanorobotic manipulation}
\subsection{2017 - A Vision-based Automated Manipulation System for the Pick-up of Carbon Nanotubes}
\begin{itemize}
\item A nanorobotic manipulation system allowing automated pick-up of carbon nanotube (CNT) based on visual feedback.
\item Histogram thresholding for automatic binarization... clearly distinguished CNTs from the substrate and other impurities under various image brightnesses and contrasts.
\item Segment detection method (SDM) to separate the CNT
and AFM cantilever during overlapping.
\item Delicate manipulation of CNTs is a large challenge because CNTs are generated in bulk.
\item Conventionally, the manipulation of nanomaterials is manually performed through teleoperation which is time consuming, highly skill dependent and unproductive. [1]
\item Visual feedback-based nanorobotic manipulation
provides an effective way to overcome these shortcomings. 
\item Great progress to visually detect the position
of such nanomaterials, such as [2] which uses the principle component analysis (PCA) to locate CNTs
\item Great efforts have been made to obtain the relative depth information from SEM such as [3]. The observation and manipulation of nano scale objects under SEM
\end{itemize}
1: \textit{2013 - Automated pickplace of silicon nanowires} \\
2: \textit{2009 - Nanolab: A nanorobotic system for automated pick-and-place handling and characterization of cnts} \\
3: \textit{2013 - A measurement method for micro 3d shape
based on grids-processing and stereovision technology}


\section{Aerial manipulation}
\subsection{2017 - Uncalibrated Visual Servo for Unmanned Aerial Manipulation}
\begin{itemize}
\item An uncalibrated image-based visual servo strategy to drive the arm end-effector to a desired position and orientation using a camera attached to it. 
\item In contrast to previous visual-servo approaches, a known value of camera focal length is not strictly required. 
\item Vision-based robot control systems are usually classified in three groups: position-based visual servo (PBVS) (the geometric model of the target is used in conjunction with image features to estimate the pose of the target with respect to the camera frame), image-based visual servo (IBVS) (both the error and control law are expressed in the image space, minimizing the error between observed and desired image feature coordinates), and hybrid control systems [1].
\item IBVS is more robust than PBVS with respect to uncertainties and disturbances affecting the model of the robot, as well as the calibration of the camera [2]
\item  Hybrid methods, also called 2-1/2-D visual servo [3], combine IBVS and PBVS to estimate partial camera displacements at each iteration of the control law minimizing a functional of both.
\item In our method, the camera focal length is iteratively estimated within the control loop.
\item In contrast to [4] which uses a combination of classical PBVS and IBVS, in this article we present a fully vision-based self-calibrated scheme that can handle poorly calibrated cameras.
\end{itemize}
1: \textit{2011 - Comparison of basic visual servoing methods} \\
2: \textit{2003 - “Robustness of image-based visual servoing with respect to depth distribution errors} \\
3: \textit{1999 - 2-1/2-D visual servoing} \\
4: \textit{2016 - Hybrid visual servoing with hierarchical task composition for aerial manipulation}


\subsection{2016 - Vision Based Autonomous Orientational Control for Aerial Manipulation via On-board FPGA}
\begin{itemize}
\item FPGA based, using camera aligning the orientation and getting close to the bar he want to grasp
\end{itemize}


\subsection{2016 - Vision-Guided Aerial Manipulation Using a Multirotor With a Robotic Arm}
\begin{itemize}
\item IBVS, multi-rotor with a multi-degree of freedom robotic arm . 
\item camera guides for having the desired velocity
\end{itemize}

\section{Humanoid} \label{humanoid}
\subsection{2016 - Vision-based manipulation with the humanoid robot Romeo}
\begin{itemize}
\item In [1] the robot HRP-2 grasps a simple ball while walking thanks to visual servoing.
\item Using the robot ARMAR-III, a hybrid approach, which combines visual estimations with orientations computed through the kinematic model, is used to control the movement of a humanoid arm in [2]. 
\item The authors of [3] present a framework for visual servo
control of the REEM robot upper body for a box grasping.
\item In [4] a vision/force coupling approach is used to guide the robot hand towards the grasp position and perform a task manipulation taking into account external forces.
\item In [5] a method for fast visual grasping of unknown objects with a multi-fingered robotic hand is described.
\item  What was included: an IBVS for gaze control, a PBVS for grasping of a generic object, a special PBVS for grasping of a cylindrical object. Some real application : box grasping, can grasping
\end{itemize}
1: \textit{2007 - Visually-guided grasping while walking on a humanoid robot} \\
2: \textit{2008 - Visual servoing for humanoid grasping and manipulation tasks} \\
3: \textit{2013 - Visual servoing for the REEM humanoid robot’s upper body} \\
4: \textit{2007 - Vision force control in task-oriented grasping and manipulation} \\
5: \textit{2013 - Visual grasp planning for unknown objects using a multifingered robotic hand}

\subsection{Charlie Rides the Elevator–Integrating Vision, Navigation and Manipulation Towards Multi-Floor Robot Locomotion}
\begin{itemize}
\item Capabilities including  robotic assistant capable of locating points of interest, manipulating objects,
\item To do so: laser range finders, stereo and monocular vision systems, and robotic arms - into a complete, task-driven autonomous system.
\item Mostly manipulation through pushing
\end{itemize}

\subsection{2015 - An Integrated Vision-based Robotic Manipulation System for Sorting Surgical Tools}
\begin{itemize}
\item Pick-up surgical tools from a tray and place the tools into different trays according to the types of the surgical tools.  
\end{itemize}



\section{Dexterous}
\subsection{2016 - Vision-based precision manipulation with underactuated hands: Simple and effective solutions for dexterity}
\begin{itemize}
\item Even though vision-based precision manipulation methods are presented in literature for fully-actuated grippers, no methods are proposed for underactuated grippers.
\item In this work we examine vision-based precision manipulation with underactuated grippers. Using vision feedback and minimal information about the underactuated mechanism, we achieved accurate positioning while maintaining a stable grasp without the use of any joint encoders or force sensors. This success is due to combining the robustness of visual servoing methods with the contact stability provided by adaptive underactuation via precision manipulation primitives (PMPs).
\end{itemize}

\section{Soft manipulation}
\subsection{2013 - Dynamic Manipulation of a Thin Circular Flexible Object using a High-Speed Multifingered Hand and High-speed Vision}
\begin{itemize}
\item Dynamic manipulation of flexible objects is needed
for factory automation and in the food industry. More specifically, we achieved stable rotational manipulation of pizza dough using a high-speed multifingered hand and high-speed visual feedback
\item  By utilizing highspeed visual feedback, it become unnecessary to estimate the deformation of the flexible object
\end{itemize}

\subsection{2012 - Vision Based Haptic Multisensor for Manipulation of Soft, Fragile Objects}
\begin{itemize}
\item a vision based multisensor that is designed for robot interaction with small, soft, and possibly fragile objects. 
\end{itemize}


\subsection{2014 - An Adaptable Robot Vision System Performing Manipulation Actions With Flexible Objects}
\begin{itemize}
\item Manipulations such as Peg-in-Hole or
Laying-Down actions
\item It is therefore required to integrate visual tracking and shape reconstruction with a physical modeling of the materials and their deformations as well as action learning techniques.
\item , an approach for tracking flexible objects directly
in the input data that does not require any form of data preprocessing (as, e.g., feature detection [1] or background subtraction [2]) or the utilization of markers [3].
\end{itemize}
1: \textit{2007 - Deformable surface tracking
ambiguities} \\
2: \textit{2013 - “Tracking deformable objects with point clouds} \\
3: \textit{2011 - “Bi-manual robotic paper manipulation
based on real-time marker tracking and physical modelling}


\section{Teleoperation}
\subsection{2016 - Vision based virtual fixture generation for teleoperated robotic manipulation}
\begin{itemize}
\item Vision in tele-operation 
\end{itemize}

\subsection{2016 - A Framework of Teleoperated and Stereo Vision Guided Mobile Manipulation for Industrial Automation}
\begin{itemize}
\item  a stereo vision system providing qualified point cloud data of the object. A modified and improved iterative closest point algorithm is applied to recognize the targeted object greatly avoiding the local minimum in template matching.
\item Moreover, a stereo vision guided teleoperation control algorithm using virtual fixtures technology is adopted to enhance robot teaching ability.
\end{itemize}

\section{Underwater}
\subsection{2016 - Underwater vehicle visual servo and target grasp control}


\section{Multi-robot}
\subsection{2016 - Image based adaptive coordinated control for cooperative manipulators}
\begin{itemize}
\item Two ways to build up a vision system: Eye in hand such as [1], and eye to hand such as [2]
\item in [3], an adaptive visual servo method is designed for trajectory tracking by decomposing the depth-independent Jacobian matrix. Here, uncertainties in kinematics and dynamics of robot manipulators are considered. 
\item visual tracking control method for the cooperative robot system is designed. 
\end{itemize}
1: \textit{2011 - A new approach to dynamic eye-in-hand visual tracking using non-linear observer} \\
2: \textit{2012 - Visual servoing of robots with uncalibrated robot and camera parameters} \\
3: \textit{2015 - Adaptive visual tracking for robotic systems without image space velocity measurement}

\section{Agriculture}
\subsection{2015 - Vision-Based High-Speed Manipulation For Robotic Ultra-Precise Weed Control}
\begin{itemize}
\item High speed image processing pipeline for closed-loop positioning of a weeding tool
\end{itemize}


\section{Assistive}
\subsection{2015 - VIBI: Assistive Vision-Based Interface for Robot Manipulation}
\begin{itemize}
\item Our proposal is to replace the direct joystick motor control interface present in a commercial wheelchair mounted assistive robotic manipulator with a human-robot interface based on visual selection.
\item  Our vision system on average was faster
than the direct joystick control in achieving an orientation
tasks. And as rated  easier. 
\item As another example [1]
\end{itemize}

1: \textit{2007 - “Simplifying wheelchair mounted robotic
arm control with a visual interface.}
2: \textit{2008 - Active rough shape estimation of unknown objects}
3: \textit{2008 - Vision-based grasping of unknown objects to improve disabled people autonomy.}
4: \textit{2012 - “Mobile manipulation through an assistive home robot}



\section{Applications}
\subsection{2009 - High-Accuracy 3D Sensing for Mobile Manipulation: Improving Object Detection and Door Opening}
\begin{itemize}
\item Object detection and door opening. Using high resolution depth information
\end{itemize}

\subsection{2010 - Autonomous operation of novel elevators for robot navigation}
\begin{itemize}
\item First time for manipulation in unknown elevator.
\end{itemize}

\subsection{2010 - Learning to open new doors}
\begin{itemize}
\item First time, autonomously opening doors with no known, 3d model of the door handle or knowledge of its location.
\end{itemize}